{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "\n",
    "动机： \n",
    "带宽,NRAM 19TB/s，dBM 1.5TB/s，快了 12.67倍，但是只有20MB可以用\n",
    "例如：\n",
    "NRAM能防止 10000 个数据元素，假设Q[100,100],Q[100,100],那么加载Q，K则需要 2x100x100 = 20000个数据，则需要对dBM进行write/read。\n",
    "\n",
    "- split Q -> [Q1[50,100] , Q2[50,100]]\n",
    "- split K -> [K1[50,100] , K1[50,100]]\n",
    "\n",
    "N_ij = Qi @ Kj, 最后想要求得完整的\n",
    "\n",
    "N = [[N00,N01] , [N10,N11]]\n",
    "\n",
    "目的：\n",
    "解决NRAM <--> dBM成为访存瓶颈，\n",
    "传统的Attn 需要7次交换，如果能拆成20MB放得下的矩阵块，就能加速7.6倍的访存时间消耗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attn原理\n",
    "\n",
    "safe softmax >> 3-pass online softmax >> 2-pass online softmax >>\n",
    "\n",
    "online softmax self-attention >> flash attn >> flash attn(tiling) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-pass online softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is tensor([0.0321, 0.0871, 0.2369, 0.6439]),x_direct is tensor([0.0321, 0.0871, 0.2369, 0.6439])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2d'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\"\"\"1d\"\"\"\n",
    "N = 4\n",
    "x = torch.arange(N,dtype = torch.float32)\n",
    "x_clone = x.clone()\n",
    "\n",
    "#更新x_max与x_sum\n",
    "x_max_old = torch.tensor(-1e6)\n",
    "x_sum_old = torch.tensor(0.0)\n",
    "for i in range(N):\n",
    "    x_max_new =  torch.max(x_max_old,x[i]) \n",
    "    x_sum_new = x_sum_old * torch.exp(x_max_old - x_max_new) + torch.exp(x[i] - x_max_new) #这里这个合并就是 2-pass\n",
    "    x_max_old = x_max_new\n",
    "    x_sum_old = x_sum_new\n",
    "#更新x\n",
    "for i in range(N):\n",
    "    x[i] = (x[i] - x_max_old).exp()/x_sum_old\n",
    "\n",
    "import torch.nn.functional as F\n",
    "x_direct = F.softmax(x_clone,dim =0)\n",
    "print(f\"x is {x},x_direct is {x_direct}\")\n",
    "assert torch.allclose(x,x_direct)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"2d\"\"\"\n",
    "# x = torch.arange(16,dtype=torch.float32).reshape(4,4)\n",
    "# x_exp  = torch.exp(x)\n",
    "# print(f\"x_exp is {x_exp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-pass self-attn >> 1-pass self-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if None:\n",
    "\n",
    "    N = 4\n",
    "    d = 2\n",
    "\n",
    "    Q = torch.arange(N*d,dtype=torch.float32).reshape(N,d)\n",
    "    K = torch.arange(N*d,dtype=torch.float32).reshape(N,d)\n",
    "    V = torch.arange(N*d,dtype=torch.float32).reshape(N,d)\n",
    "\n",
    "    for i in range(N): #外圈 K,V\n",
    "        # for k in range(): #内圈Q \n",
    "        x_i = Q[k,:] @ K[:,i] #k是啥\n",
    "        m_i = max(m_i,x_i) #最大值\n",
    "        d_i = d_i * torch.exp(m_(i-1) - m_i) + torch.exp(x_i - m_i) #和\n",
    "    for i in range(N):\n",
    "        a_i =  torch.exp(x_i) - m_i / d_N\n",
    "        o_i = o_(i-1) + a_i*V[i,:]\n",
    "\n",
    "#1-pass flash-attn \n",
    "\n",
    "    for i in range(N): #外圈 K,V\n",
    "        x_i = Q[k,:] @ K[:,i] #k是啥\n",
    "        m_i = max(m_i,x_i) #最大值\n",
    "        d_i = d_i * torch.exp(m_(i-1) - m_i) + torch.exp(x_i - m_i) #和\n",
    "\n",
    "        o_i = d(i-1) * torch.exp(m_(i-1) - m_i) /d_i + torch.exp(x_i - m_i)/d_i * V[i,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash-attn实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S.shape is torch.Size([1, 1, 6, 6])\n",
      "O.shape is torch.Size([1, 1, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "#Standard self-attn\n",
    "import torch\n",
    "\n",
    "NEG_INF = -1e10\n",
    "EPSILON = 1e-10\n",
    "\n",
    "\"\"\"\n",
    "SRAM size is M\n",
    "\"\"\"\n",
    "#原始的 QKV尺寸  6x4 \n",
    "N = 6\n",
    "d = 2\n",
    "#创建 QKV，Olm等矩阵\n",
    "Q = torch.randn(1,1,N,d,requires_grad=True)\n",
    "K = torch.randn(1,1,N,d,requires_grad=True)\n",
    "V = torch.randn(1,1,N,d,requires_grad=True)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "S = F.softmax(torch.einsum('...id,...jd -> ... ij',Q,K),dim = -1)\n",
    "print(f\"S.shape is {S.shape}\")\n",
    "O = torch.einsum('...Nk,...kd -> ... Nd',S,V)\n",
    "print(f\"O.shape is {O.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l.shape is torch.Size([1, 1, 6, 1]),m.shape is torch.Size([1, 1, 6, 1])\n",
      "(i,j) is (0,0),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (1,0),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (2,0),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (0,1),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (1,1),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (2,1),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (0,2),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (1,2),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "(i,j) is (2,2),S_ij.shape is torch.Size([1, 1, 2, 2])\n",
      "O.shape is torch.Size([1, 1, 6, 2])\n",
      "Od.shape is torch.Size([1, 1, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "NEG_INF = -1e10\n",
    "EPSILON = 1e-10\n",
    "\n",
    "\"\"\"\n",
    "SRAM size is M\n",
    "\"\"\"\n",
    "#原始的 QKV尺寸  6x4 \n",
    "N = 6\n",
    "d = 2\n",
    "\n",
    "#切分求解 block_size ,Bc = M/4d,Br = min(M/4d,d)\n",
    "Q_block_size = 2 #6//2,\n",
    "KV_block_size = 2\n",
    "#根据block_size得到，Q,K,V 切出来，Tr行和Tc列\n",
    "Tr = N//Q_block_size\n",
    "Tc = N//KV_block_size \n",
    "\n",
    "#创建 QKV，Olm等矩阵\n",
    "Q = torch.randn(1,1,N,d,requires_grad=True)\n",
    "K = torch.randn(1,1,N,d,requires_grad=True)\n",
    "V = torch.randn(1,1,N,d,requires_grad=True)\n",
    "\n",
    "O = torch.zeros_like(Q, requires_grad=True)\n",
    "l = torch.zeros(Q.shape[:-1])[..., None] #删减后在增减一个为1的维度 \n",
    "m = torch.ones(Q.shape[:-1])[..., None] * NEG_INF\n",
    "print(f\"l.shape is {l.shape},m.shape is {m.shape}\")\n",
    "\n",
    "#切分成tiling\n",
    "Q_blocks = torch.split(Q,Q_block_size,dim = 2) #沿着序列维度去切分他,变成两个元组了，这里split第二个参数是 尺寸大小 \n",
    "# print(f\"Q_blocks is {Q_blocks}\") \n",
    "K_blocks = torch.split(K,KV_block_size,dim = 2)\n",
    "V_blocks = torch.split(V,KV_block_size,dim = 2)\n",
    "\n",
    "O_blocks = list(torch.split(O,Q_block_size,dim = 2))\n",
    "l_blocks = list(torch.split(l,Q_block_size,dim = 2))\n",
    "m_blocks = list(torch.split(m,Q_block_size,dim = 2))\n",
    "\n",
    "for j in range(Tc): #列循环 \n",
    "    Kj = K_blocks[j] #加载进来 K和V\n",
    "    Vj = V_blocks[j]\n",
    "    for i in range(Tr): #行循环 \n",
    "        Qi = Q_blocks[i]\n",
    "        Oi = O_blocks[i]\n",
    "        li = l_blocks[i]\n",
    "        mi = m_blocks[i]\n",
    "\n",
    "        S_ij = torch.einsum('... i d, ... j d -> ... i j',Qi,Kj) #einsum和@有什么区别？\n",
    "        print(f\"(i,j) is ({i},{j}),S_ij.shape is {S_ij.shape}\")\n",
    "        #对S解决 batch-block-online-softmax\n",
    "        m_block_ij,_ = torch.max(S_ij,dim = -1,keepdims = True) #行最大值\n",
    "        P_ij = torch.exp(S_ij - m_block_ij) #求个exp(x-m),P_ij就只是把S_ij转换成概率\n",
    "        l_block_ij = torch.sum(P_ij,dim = -1,keepdim = True) + EPSILON #行求和，防止是0所以加个极小值，要做除数 \n",
    "\n",
    "        #更新行的最大值 与 和\n",
    "        mi_new = torch.maximum(m_block_ij,mi) #行最大值 与 同行的第j列求最大值 \n",
    "        li_new = li * torch.exp(mi-mi_new) + torch.exp(m_block_ij - mi_new) * l_block_ij #行前和修正 + 当前块修正\n",
    "        \n",
    "        #求解Oi，Oi修正 + P_ij_Vj修正(这里 P_ij_Vj代表exp(Sij)和Vj直接乘起来了)\n",
    "        P_ij_Vj = torch.einsum('...ij,...jd -> ...id',P_ij,Vj) #与Vj乘起来，但是后面需要修正mi以及除以li_new\n",
    "        O_blocks[i] =  (li/li_new) * torch.exp(mi - mi_new) * Oi + (torch.exp(m_block_ij - mi_new) / li_new) * P_ij_Vj\n",
    "\n",
    "        l_blocks[i] = li_new\n",
    "        m_blocks[i] = mi_new\n",
    "\n",
    "O = torch.cat(O_blocks,dim = 2)\n",
    "l = torch.cat(l_blocks,dim = 2)\n",
    "m = torch.cat(m_blocks,dim = 2)\n",
    "\n",
    "print(f\"O.shape is {O.shape}\")        \n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "Sd = F.softmax(torch.einsum('...id,...jd -> ... ij',Q,K),dim = -1)\n",
    "Od = torch.einsum('...Nk,...kd -> ... Nd',Sd,V)\n",
    "print(f\"Od.shape is {Od.shape}\")\n",
    "\n",
    "assert torch.allclose(Od,O,atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
